{\rtf1\ansi\ansicpg1252\cocoartf2821
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww28600\viewh17440\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 # Topic Modeling of U.S. Presidential Speeches: LDA vs. BERTopic\
\
## 1. Project Overview\
\
This project performs topic modeling on a manually collected corpus of U.S. Presidential speeches to uncover latent thematic structures within the discourse. It applies and compares two popular topic modeling techniques: Latent Dirichlet Allocation (LDA) and BERTopic. The analysis includes data collection, extensive preprocessing, exploratory data analysis (EDA), model training, visualization, and a comparative discussion of the results obtained from both methods. The implementation was tested on both local (Linux via Conda/venv) and cloud (Google Colab) environments.\
\
## 2. Features\
\
*   **Data Collection:** Corpus manually collected from sources like The American Presidency Project and the Miller Center.\
*   **Preprocessing:** Includes HTML tag removal, basic text cleaning (lowercase, remove special chars/numbers), spaCy-based lemmatization, and custom/standard stopword removal.\
*   **Exploratory Data Analysis (EDA):** Analysis of speech distribution over time, per president, transcript lengths, and initial word frequencies.\
*   **LDA Topic Modeling:** Implementation using Gensim's `LdaMulticore`.\
*   **BERTopic Modeling:** Implementation using the `bertopic` library, leveraging Sentence-BERT embeddings, UMAP, and KMeans clustering (to match LDA topic count).\
*   **Visualization:**\
    *   Standard EDA plots (histograms, bar charts).\
    *   LDA topic visualization using `pyLDAvis` (with t-SNE).\
    *   BERTopic visualizations (`visualize_topics`, `visualize_barchart`, `visualize_hierarchy`, `visualize_heatmap`).\
    *   Custom Plotly charts for side-by-side keyword comparison between LDA and BERTopic.\
*   **Comparative Analysis:** Qualitative comparison of topic coherence, distinctiveness, and visualization clarity between LDA and BERTopic.\
*   **Environment Comparison:** Notes on performance differences (training time) between local and cloud execution.\
\
## 3. Dataset\
\
*   **Source:** Manually collected and collated U.S. Presidential speeches, messages, and addresses from online archives such as The American Presidency Project (presidency.ucsb.edu) and the Miller Center (millercenter.org). **This is NOT a standard pre-packaged dataset.**\
*   **Content:** Full transcripts of speeches.\
*   **Timeframe:** Approximately 1790 - 2024.\
*   **Size:** 1061 documents.\
*   **Format:** Assumed to be in a CSV file (e.g., `speeches_corpus.csv`) with columns like `transcript`, `president`, `date`, `title`.\
\
\
## 5. Installation & Setup\
\
1.  **Clone Repository (if applicable):**\
    ```bash\
    git clone [Your Repository URL]\
    cd [Your Repository Directory]\
    ```\
2.  **Python Version:** Recommended Python 3.9 or higher.\
3.  **Virtual Environment:** It is highly recommended to use a virtual environment:\
    *   Using `venv`:\
        ```bash\
        python -m venv venv\
        source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\
        ```\
    *   Using `conda`:\
        ```bash\
        conda create -n topicenv python=3.9\
        conda activate topicenv\
        ```\
4.  **Install Dependencies:** Create a `requirements.txt` file (if you don't have one) in your local environment using `pip freeze > requirements.txt` after installing all packages. Then install using:\
    ```bash\
    pip install -r requirements.txt\
    ```\
    **Key dependencies include:**\
    *   `pandas`\
    *   `numpy`\
    *   `nltk`\
    *   `spacy`\
    *   `gensim`\
    *   `scikit-learn`\
    *   `bertopic`\
    *   `sentence-transformers`\
    *   `umap-learn`\
    *   `hdbscan` (install even if using KMeans, sometimes a dependency)\
    *   `plotly`\
    *   `pyLDAvis`\
    *   `textblob`\
    *   `wordcloud`\
    *   `matplotlib`\
    *   `seaborn`\
    *(Ensure your `requirements.txt` has the correct versions)*\
5.  **Download NLTK Data:**\
    ```python\
    import nltk\
    nltk.download('stopwords')\
    nltk.download('wordnet')\
    nltk.download('omw-1.4')\
    ```\
6.  **Download spaCy Model:**\
    ```bash\
    python -m spacy download en_core_web_sm\
    ```\
\
## 6. Usage\
\
1.  **Place Data:** Ensure your collected dataset (e.g., `speeches_corpus.csv`) is in the correct directory as expected by the script/notebook.\
2.  **Run Analysis:**\
    *   If using a Jupyter Notebook (e.g., `main_analysis_notebook.ipynb`): Open and run the cells sequentially.\
    *   If using a Python script (e.g., `run_analysis.py`): Execute from the command line: `python run_analysis.py`\
3.  **Outputs:** The script/notebook will perform EDA, preprocessing, train both LDA and BERTopic models, generate visualizations (some displayed inline, some potentially saved to files or HTML), and print keyword comparisons. Saved models or visualizations might appear in `models/` or `visualizations/` folders if saving code was implemented.\
\
## 7. Key Libraries & Versions (Example)\
\
Please update with versions from your `requirements.txt`:\
\
*   Python: `3.9.x`\
*   pandas: `2.x.x`\
*   nltk: `3.8.x`\
*   spacy: `3.x.x` (`en_core_web_sm` model)\
*   gensim: `4.3.x`\
*   scikit-learn: `1.x.x`\
*   bertopic: `0.16.x`\
*   sentence-transformers: `2.x.x`\
*   umap-learn: `0.5.x`\
*   plotly: `5.x.x`\
*   pyLDAvis: `3.4.x`\
\
## 8. Results Summary\
\
The analysis generated 10 distinct topics using both LDA and BERTopic models. Keyword analysis and visualizations (inter-topic distance maps, keyword bar charts, hierarchy plots, heatmaps) were used to interpret these topics and compare the models' outputs. A detailed comparison focusing on semantic coherence, topic structure, and visualization clarity is available in the full project report.\
\
## 9. Cloud vs. Local Execution\
\
As required by the project brief, the code was executed in both Google Colab (cloud) and a local Linux environment. Performance comparisons (primarily model training time) were recorded. BERTopic training time significantly benefited from GPU acceleration available on Colab, while LDA performance was more dependent on CPU resources. See the full report for detailed timings and discussion.\
\
## 10. Author\
Odera Nnaji}