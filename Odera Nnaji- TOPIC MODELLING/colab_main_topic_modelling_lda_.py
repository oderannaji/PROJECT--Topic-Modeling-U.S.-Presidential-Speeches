# -*- coding: utf-8 -*-
"""colab Main Topic modelling LDA .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1urMM0OqEPl7MA9BDlX1u6ls_htHRU0gI
"""

# Commented out IPython magic to ensure Python compatibility.
# Record Each Cell's Execution Time
!pip install ipython-autotime
# %load_ext autotime

!pip install requests gensim

from google.colab import drive
drive.mount('/content/drive')

# Then load the JSON file
import json

try:
    # Adjust the path if your file is in a subfolder
    with open('/content/drive/MyDrive/presidential_speeches.json', 'r') as f:
        all_speeches = json.load(f)
    print(f"Successfully loaded {len(all_speeches)} speeches!")

    # Quick verification
    print("\nFirst speech sample:")
    print(f"President: {all_speeches[0]['president']}")
    print(f"Title: {all_speeches[0]['title']}")
    print(f"Date: {all_speeches[0]['date']}")
    print(f"Preview: {all_speeches[0]['transcript'][:100]}...")

except FileNotFoundError:
    print("Error: File not found. Please check:")
    print("1. The file name is correct (case-sensitive)")
    print("2. The file is in your Drive's root folder")
    print("3. You've mounted Drive correctly")
except json.JSONDecodeError:
    print("Error: File is not valid JSON")
except Exception as e:
    print(f"Unexpected error: {str(e)}")

#!pip install --upgrade pandas

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
!pip  install textblob
from textblob import TextBlob

# Data Cleaning
import numpy as np
import json
import re
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Topic Modeling Prep
import gensim
from gensim.utils import simple_preprocess
from gensim import models, matutils

from sklearn.feature_extraction.text import CountVectorizer
import spacy
import random
from collections import Counter

!pip install pyLDAvis
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis.gensim_models
import pyLDAvis
from gensim.corpora import Dictionary

df = pd.DataFrame(all_speeches)
# Display first few rows
df.head(10)

# --- EDA - Initial Inspection ---

# 1. Get a quick overview of the data types and non-null values
df.info()

# Generate descriptive statistics to understand data distribution
df.describe(include='all')

# Clean and convert dates
df['date'] = (
    df['date']
    .astype(str)  # Convert all values to strings first
    .str.split('T').str[0]  # Remove timezone portion
    .pipe(pd.to_datetime, errors='coerce')  # Convert to datetime
)

# Verify conversion
print("Date column type:", df['date'].dtype)
print("Null dates:", df['date'].isna().sum())

# Check sample dates
print("\nFirst 5 dates:")
print(df['date'].head())

# Year distribution
df['date'].dtype == 'datetime64[ns]'
df['year'] = df['date'].dt.year
plt.figure(figsize=(12,6))
df['year'].hist(bins=30)
plt.title('Distribution of Speeches by Year')
plt.show()

# EDA - Date Exploration

# 1. Convert 'date' column to datetime objects with UTC=True to handle timezones
df['date'] = pd.to_datetime(df['date'], format='ISO8601', errors='coerce', utc=True)

# 2. Verify the conversion and data types
print(df.info())


# Check for null dates after conversion
print(f"Number of null dates = {df['date'].isnull().sum()}")


# 3. Extract year for easier yearly analysis (only for valid dates)
df['year'] = df['date'].dt.year

# 4. Visualize speech count per year using a line plot
print("--- Speech Count per Year - Line Plot ---")
yearly_counts = df['year'].value_counts().sort_index()
plt.figure(figsize=(12, 6))
sns.lineplot(x=yearly_counts.index, y=yearly_counts.values)
plt.title('Number of Presidential Speeches per Year')
plt.xlabel('Year')
plt.ylabel('Number of Speeches')
plt.grid(True)
plt.show()
print("\n")

# Investigate documents with duplicated dates
duplicate_dates_df = df[df.duplicated(subset=['date'], keep=False)].sort_values(by='date')
print("--- Documents with Duplicated Dates (Sorted by Date) ---")
duplicate_dates_df.head(5)

# --- EDA - Transcript Length Analysis ---

# 1. Calculate the length of each transcript (word count)
df['transcript_length'] = df['transcript'].apply(lambda text: len(text.split()))

# 2. Get descriptive statistics of transcript lengths
print("--- Descriptive Statistics of Transcript Lengths (df['transcript_length'].describe()) ---")
print(df['transcript_length'].describe())
print("\n")

# 3. Visualize the distribution of transcript lengths using a histogram
print("--- Histogram of Transcript Lengths ---")
plt.figure(figsize=(10, 6)) # Adjust figure size
sns.histplot(df['transcript_length'], bins=50, kde=True) # bins=50 for reasonable detail, kde=True for density curve
plt.title('Distribution of Presidential Speech Lengths (Word Count)')
plt.xlabel('Transcript Length (Words)')
plt.ylabel('Frequency (Number of Speeches)')
plt.grid(axis='y', alpha=0.75) # Grid on y-axis for readability
plt.show()
print("\n")

# Find the row with the maximum transcript length
longest_speech_row = df[df['transcript_length'] == df['transcript_length'].max()]
longest_speech_row

# Find the 5 rows with the shortest transcript lengths
shortest_speeches_df = df.nsmallest(5, 'transcript_length')
shortest_speeches_df

# President analysis
plt.figure(figsize=(25, 9))

# Get value counts and sort them
president_counts = df['president'].value_counts()

# Plot with explicit index alignment
ax = president_counts.plot(kind='bar')

# Set title and labels
plt.title('Number of Speeches per President', fontsize=16)
plt.ylabel('Count', fontsize=12)

# Fix alignment using the correct parameter name and explicit positioning
plt.xticks(
    ticks=range(len(president_counts)),  # Explicit x-tick positions
    labels=president_counts.index,        # President names from the index
    rotation=35,                          # Correct parameter name (not rotations)
    ha='right',                           # Horizontal alignment
    fontsize=10
)

# Adjust layout to prevent label cutoff
plt.tight_layout()
plt.show()

# Add text length features
df['word_count'] = df['transcript'].apply(lambda x: len(x.split()))
df['char_count'] = df['transcript'].apply(len)

# Text length distributions
fig, ax = plt.subplots(1,2, figsize=(15,5))
df['word_count'].hist(ax=ax[0], bins=30)
ax[0].set_title('Word Count Distribution')
df['char_count'].hist(ax=ax[1], bins=30)
ax[1].set_title('Character Count Distribution')
plt.show()

# Most common words before preprocessing
all_words = ' '.join(df['transcript']).split()
common_words = Counter(all_words).most_common(60)
print("Top 60 Common Words:")
common_words

# Get the least common words
least_common_words = Counter(all_words).most_common()[-90:]
print("30 Least Common Words:")
least_common_words

# Stopword analysis
stop_words = set(stopwords.words('english'))
stopword_count = sum(1 for word in all_words if word.lower() in stop_words)
print(f"\nStopwords make up {stopword_count/len(all_words):.1%} of all words")

# Decade-based analysis
df['decade'] = (df['year'] // 10) * 10
decade_counts = df.groupby('decade')['transcript'].count()

plt.figure(figsize=(12,6))
decade_counts.plot(kind='bar')
plt.title('Speeches per Decade')
plt.xlabel('Decade')
plt.ylabel('Number of Speeches')
plt.show()

# President vs text length
plt.figure(figsize=(12,6))
df.groupby('president')['word_count'].mean().sort_values(ascending=False).plot(kind='bar')
plt.title('Average Speech Length by President')
plt.ylabel('Average Word Count')
plt.show()

# Add sentiment scores
df['sentiment'] = df['transcript'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Sentiment over time
plt.figure(figsize=(12,6))
df.groupby('year')['sentiment'].mean().plot()
plt.title('Average Sentiment Score Over Time')
plt.ylabel('Sentiment Polarity')
plt.show()

# Step 1: Data Cleaning
# Keep only relevant columns
df = df[['transcript', 'president', 'date']]  # Keep president/date for later analysis

# Step 2: Text Preprocessing
def clean_text(text):
    # Remove special characters and numbers
    text = re.sub(r'[^a-zA-Z\s]', '', text, re.I|re.A)
    # Convert to lowercase
    text = text.lower()
    # Remove extra whitespace
    text = re.sub('\s+', ' ', text)
    return text

df['clean_text'] = df['transcript'].apply(clean_text)

# Step 3: Word Cloud Visualization
# Join all texts
all_text = ' '.join(df['clean_text'])

clean_text(df['transcript'][0])



# Load spaCy model for lemmatization
nlp = spacy.load('en_core_web_sm')

# 1. Define a custom stop word list - extending the default English list
custom_stop_words = list(CountVectorizer(stop_words='english').get_stop_words()) # Start with default English stop words
custom_stop_words.extend([
    'states', 'united', 'government', 'congress', 'president', 'year', 'years', 'time', 'great', 'new', 'public', 'state', 'nation', 'american', 'america'
])
custom_stop_words_list = list(custom_stop_words)  # Convert set to list for CountVectorizer compatibility

# 2. Function to remove HTML tags
def remove_html_tags(text):
    clean_text = re.sub(r'<[^>]+>', '', text)
    return clean_text

# 3. Lemmatize text
def lemmatize_text(text):
    doc = nlp(text)
    lemmatized_words = [token.lemma_ for token in doc]
    return " ".join(lemmatized_words)

# 4. Sentence to words
def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations

# 5. Remove stopwords
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc))
             if word not in custom_stop_words_list] for doc in texts]

# Assuming df is your DataFrame with a 'text' column containing the raw text
# Apply HTML tag removal first to the transcript column
df['clean_text'] = df['transcript'].apply(remove_html_tags)  # Process raw transcripts

# Apply lemmatization
df['clean_text'] = df['clean_text'].apply(lemmatize_text)

# Convert to list
data = df['clean_text'].values.tolist()

# Tokenization
data_words = list(sent_to_words(data))

# Remove stopwords FIRST PASS
data_words = remove_stopwords(data_words)

# Add processed tokens to DataFrame
df['processed_tokens'] = data_words

# Create a readable version of processed text
df['processed_text'] = df['processed_tokens'].apply(lambda x: ' '.join(x))

# Compare original vs processed text
pd.set_option('display.max_colwidth', 500)  # Show full text width

# View sample comparisons
sample = df.sample(3, random_state=42)[['transcript', 'processed_text']]
print("\nOriginal vs Processed Text Samples:")
display(sample)

num_samples = 3
for idx in range(num_samples):
    print(f"\n=== Speech {idx+1} ===")
    print(f"Original:\n{df['transcript'].iloc[idx][:500]}...")
    print(f"\nProcessed:\n{df['processed_text'].iloc[idx][:500]}...")
    print("\n" + "-"*80)

president_name = "Abraham Lincoln"
lincoln_speeches = df[df['president'] == president_name][['transcript', 'processed_text']]
display(lincoln_speeches.head())

def show_token_comparison(row_idx):
    original = df['transcript'].iloc[row_idx].lower().split()
    processed = df['processed_tokens'].iloc[row_idx]

    print("Original Tokens:", original[:20])
    print("Processed Tokens:", processed[:20])

show_token_comparison(0)  # First speech
show_token_comparison(random.randint(0, len(df)))  # Random speech

# 1. Flatten the token lists for unigram analysis
all_tokens = [token for sublist in df['processed_tokens'] for token in sublist]

# 2. Generate n-grams

def get_ngrams(tokens, n):
    return list(zip(*[tokens[i:] for i in range(n)]))

# Get all bigrams and trigrams
bigrams = get_ngrams(all_tokens, 2)
trigrams = get_ngrams(all_tokens, 3)

# 3. Count n-gram frequencies
unigram_counts = Counter(all_tokens)
bigram_counts = Counter(bigrams)
trigram_counts = Counter(trigrams)

# 4. Create DataFrames for each n-gram
def create_ngram_df(counts, n):
    df_ngram = pd.DataFrame(counts.most_common(20), columns=[f'{" ".join(["Word"]*n)}', 'Count'])
    df_ngram[f'{" ".join(["Word"]*n)}'] = df_ngram[f'{" ".join(["Word"]*n)}'].apply(
        lambda x: ' '.join(x) if isinstance(x, tuple) else x)
    return df_ngram

uni_df = create_ngram_df(unigram_counts, 1)
bi_df = create_ngram_df(bigram_counts, 2)
tri_df = create_ngram_df(trigram_counts, 3)

# 5. Visualization
plt.figure(figsize=(15, 15))

# 6. Display tables
print("\nTop 20 Unigrams:")
display(uni_df.head(20))

# Unigrams
plt.subplot(3, 1, 1)
sns.barplot(x='Count', y='Word', data=uni_df, palette='viridis')
plt.title('Top 20 Unigrams')
plt.xlabel('Count')
plt.ylabel('Word')

# Bigrams
plt.subplot(3, 1, 2)
sns.barplot(x='Count', y='Word Word', data=bi_df, palette='magma')
plt.title('Top 20 Bigrams')
plt.xlabel('Count')
plt.ylabel('Word Pair')

# Trigrams
plt.subplot(3, 1, 3)
sns.barplot(x='Count', y='Word Word Word', data=tri_df, palette='plasma')
plt.title('Top 20 Trigrams')
plt.xlabel('Count')
plt.ylabel('Word Triplet')

plt.tight_layout()
plt.show()

# 6. Display tables
print("\nTop 20 Unigrams:")
display(uni_df.head(20))

print("\nTop 20 Bigrams:")
display(bi_df.head(20))

print("\nTop 20 Trigrams:")
display(tri_df.head(20))

# Combine all tokens into a single string
all_text = ' '.join([' '.join(tokens) for tokens in df['processed_tokens']])

# Generate the word cloud
wordcloud = WordCloud(width=1200,
                     height=600,
                     background_color='white',
                     max_words=150,
                     colormap='viridis',
                     collocations=True).generate(all_text)

# Display the visualization
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title("Most Significant Terms in Text", fontsize=18, pad=20)
plt.axis('off')
plt.tight_layout()
plt.show()

# df['clean_text'] - Your final processed text column
# data_words - Your tokenized text after stopword removal
# custom_stop_words_list - Your predefined stopwords

print("--- LDA Model Training ---")

# 1. Convert tokens back to text (matching your preprocessing)
texts_for_lda = [' '.join(tokens) for tokens in data_words]

# 2. Initialize CountVectorizer with YOUR stopwords
vectorizer_lda = CountVectorizer(
    stop_words=custom_stop_words_list,
    token_pattern=r'\b[a-zA-Z]{3,}\b',
    ngram_range=(1, 1)
)

# 3. Create document-term matrix from clean_text
X_lda = vectorizer_lda.fit_transform(df['clean_text'])
print(f"Document-term matrix shape: {X_lda.shape}")

# 4. Prepare Gensim objects (using your variable names)
id2word = {idx: word for word, idx in vectorizer_lda.vocabulary_.items()}
corpus = matutils.Sparse2Corpus(X_lda, documents_columns=False)

# 5. Train LDA model (same parameters as yours)
lda_model = models.LdaMulticore(
    corpus=corpus,
    id2word=id2word,
    num_topics=10,
    random_state=100,
    passes=10,
    per_word_topics=True
)

# 6. Print topics (improved formatting)
print("\n--- Discovered Topics ---")
for topic in lda_model.print_topics(num_words=10):
    print(f"\nTopic {topic[0]}:")
    print(topic[1])

#pip install -U pyLDAvis gensim pandas joblib scikit-learn

# Convert your id2word to proper Gensim Dictionary format
gensim_dict = Dictionary.from_corpus(corpus, id2word=id2word)

lda_visualization = pyLDAvis.gensim_models.prepare(
    lda_model,
    corpus,
    gensim_dict,
    sort_topics=False,
    mds='tsne'
)

pyLDAvis.display(lda_visualization)

import matplotlib.pyplot as plt
import pandas as pd
import math

# --- Visualize LDA Topic Keywords ---
print("Generating LDA Keyword Bar Charts (Similar Style to BERTopic)...")

# Ensure lda_model and id2word (or vectorizer_lda) are available from your LDA run

# How many topics and words to show
num_topics_lda = lda_model.num_topics # Or set explicitly to 10 if needed
num_words_lda = 10 # Show top 10 words

# Get topic terms and probabilities
topics_terms = lda_model.show_topics(num_topics=num_topics_lda,
                                    num_words=num_words_lda,
                                    formatted=False) # Get (word, probability) pairs

# Determine grid size for subplots
cols = 2 # Arrange in 2 columns
rows = math.ceil(num_topics_lda / cols)
fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 4), sharex=True)
axes = axes.flatten()

for i, topic_data in enumerate(topics_terms):
    topic_id, terms = topic_data

    # Create DataFrame for plotting
    terms_df = pd.DataFrame(terms, columns=['Term', 'Probability'])

    # Plot on the appropriate subplot axis
    ax = axes[i]
    sns.barplot(ax=ax, x='Probability', y='Term', data=terms_df, palette='viridis', orient='h') # Use orient='h'
    ax.set_title(f"LDA Topic {topic_id}")
    ax.set_xlabel("Term Probability")
    ax.set_ylabel("") # Remove redundant Y label

# Hide any unused subplots
for j in range(i + 1, len(axes)):
    axes[j].set_visible(False)

plt.suptitle("Top Terms per LDA Topic", y=1.02) # Add a main title
plt.tight_layout()
plt.show()

print("\nLDA Keyword Bar Charts generated.")





"""# BERT"""

#!pip install bertopic umap-learn hdbscan scikit-learn sentence-transformers plotly

import pandas as pd
from bertopic import BERTopic
from umap import UMAP
# We will use KMeans
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import CountVectorizer


if 'clean_text' not in df.columns:
     raise ValueError("DataFrame missing the 'clean_text' column with lemmatized text.")

print("Preparing data for BERTopic...")
docs_for_bertopic = df['clean_text'].tolist()

# Check data
if not docs_for_bertopic or not isinstance(docs_for_bertopic[0], str):
     raise ValueError("Input 'docs_for_bertopic' is not a valid list of strings.")

print(f"Number of documents for BERTopic: {len(docs_for_bertopic)}")
print("Sample document (first 500 chars):")
print(docs_for_bertopic[0][:500])

# Configure BERTopic Components
print("\nSetting up BERTopic components...")

# Dimensionality Reduction Model (match LDA random state)
umap_model_bert = UMAP(n_neighbors=15,
                       min_dist=0.0,
                       metric='cosine',
                       random_state=100) # Match LDA random state

# Clustering Model (Using KMeans)
num_target_topics = 10
kmeans_model_bert = KMeans(n_clusters=num_target_topics,
                           random_state=100,
                           n_init='auto') # Set n_init explicitly

# Keyword Extraction Model (Using n_gram_range)
vectorizer_model_bert = CountVectorizer(stop_words="english", ngram_range=(1, 3))


# Embedding Model (Using BERTopic's default: "all-MiniLM-L6-v2")

#Initialize and Fit BERTopic Model
print(f"\nInitializing BERTopic model with KMeans (k={num_target_topics})...")
topic_model_bertopic = BERTopic(
                    umap_model=umap_model_bert,
                    hdbscan_model=kmeans_model_bert, # Pass the KMeans model here
                    vectorizer_model=vectorizer_model_bert,
                    language="english",
                    calculate_probabilities=True,
                    verbose=True
                 )

print("\nFitting BERTopic model... This can take some time.")
topics_bert, probs_bert = topic_model_bertopic.fit_transform(docs_for_bertopic)

print("\nBERTopic model fitting complete.")

# Display Basic Results
# With KMeans, there should be exactly num_target_topics (e.g., 10) and no Topic -1
topic_info_bert = topic_model_bertopic.get_topic_info()
print("\nBERTopic Model Info (using KMeans):")
print(topic_info_bert)

print("\nKeywords for first few topics found:")
# Print top 5 topics found (should be 0-4 if num_target_topics=10)
num_topics_to_show = min(5, len(topic_info_bert))
for i in range(num_topics_to_show):
    topic_id = topic_info_bert.iloc[i]['Topic']
    print(f"Topic {topic_id}: {topic_model_bertopic.get_topic(topic_id)}")

# Add topic predictions to your DataFrame
df['bertopic_topic'] = topics_bert
print("\nAdded BERTopic topic assignments to DataFrame column 'bertopic_topic'.")
print(df[['clean_text', 'bertopic_topic']].head())

# Generate BERTopic Visualizations
!pip install plotly

# 1. Visualize top topic keywords
num_topics_found = len(topic_info_bert)
print(f"\n--- Bar Chart of Top Keywords for {num_topics_found} Topics ---")
fig1 = topic_model_bertopic.visualize_barchart(top_n_topics=num_topics_found)
fig1.show()

# Visualize term rank decrease (Shows importance across topics)
print("\n--- Term Rank Visualization ---")
fig2 = topic_model_bertopic.visualize_term_rank()
fig2.show()

# Visualize intertopic distance map (like pyLDAvis map)
print("\n--- Intertopic Distance Map (2D Projection) ---")
fig3 = topic_model_bertopic.visualize_topics()
fig3.show()

# Visualize connections between topics using hierarchical clustering
print("\n--- Hierarchical Clustering Dendrogram ---")
fig4 = topic_model_bertopic.visualize_hierarchy(top_n_topics=num_topics_found)
fig4.show()

# Visualize similarity using heatmap
print("\n--- Topic Similarity Heatmap ---")
fig5 = topic_model_bertopic.visualize_heatmap(top_n_topics=num_topics_found)
fig5.show()

print("\nVisualizations generated.")



import pandas as pd
# Make sure lda_model and topic_model_bertopic are your trained models
# Make sure id2word (for LDA) is available

print("Extracting Top 10 Keywords for Each Topic from Both Models...")

num_keywords = 10
lda_topics = {}
bertopic_topics = {}

# Assuming LDA model has topics 0-9 (based on num_topics=10)
for i in range(10):
    # LDA: Get words and probabilities
    lda_raw = lda_model.show_topic(i, topn=num_keywords)
    # Convert Gensim format (word, prob) to dictionary {word: prob}
    lda_topics[i] = {word: prob for word, prob in lda_raw}

    # BERTopic: Get words and c-TF-IDF scores
    # Need to handle potential errors if KMeans created fewer topics than 10
    try:
        bertopic_raw = topic_model_bertopic.get_topic(i)
        # Ensure we got data and convert (word, score) to dictionary {word: score}
        if bertopic_raw:
             bertopic_topics[i] = {word: score for word, score in bertopic_raw}
        else:
             print(f"Warning: BERTopic did not produce keywords for Topic {i}")
             bertopic_topics[i] = {} # Assign empty dict if topic doesn't exist in BERTopic output
    except ValueError: # Handle case where topic ID might not exist if KMeans failed unexpectedly
        print(f"Warning: BERTopic Topic ID {i} not found.")
        bertopic_topics[i] = {}


# Display side-by-side for comparison

comparison_data = []
for topic_id in range(10):
    lda_words = list(lda_topics.get(topic_id, {}).keys())
    lda_scores = list(lda_topics.get(topic_id, {}).values())
    bertopic_words = list(bertopic_topics.get(topic_id, {}).keys())
    bertopic_scores = list(bertopic_topics.get(topic_id, {}).values())

    # Pad shorter lists if necessary for DataFrame creation (though both should have 10)
    max_len = num_keywords
    lda_words += [''] * (max_len - len(lda_words))
    lda_scores += [0.0] * (max_len - len(lda_scores))
    bertopic_words += [''] * (max_len - len(bertopic_words))
    bertopic_scores += [0.0] * (max_len - len(bertopic_scores))

    comparison_data.append({
        'Topic ID': topic_id,
        'LDA Keywords': lda_words,
        'LDA Scores': [f"{s:.4f}" for s in lda_scores], # Format scores
        'BERTopic Keywords': bertopic_words,
        'BERTopic Scores': [f"{s:.4f}" for s in bertopic_scores] # Format scores
    })

comparison_df = pd.DataFrame(comparison_data)

# Display the comparison table (might need adjustment for readability in some environments)
print("\n--- Keyword Comparison Table (Top 10) ---")
# Transpose for better readability sometimes, or display directly
# Option 1: Direct display
# display(comparison_df)

# Option 2: Print topic by topic
for index, row in comparison_df.iterrows():
    print(f"\n----- Topic {row['Topic ID']} -----")
    print("LDA Keywords:          ", ", ".join(row['LDA Keywords']))
    # print("LDA Scores:          ", ", ".join(row['LDA Scores']))
    print("BERTopic Keywords:     ", ", ".join(row['BERTopic Keywords']))
    # print("BERTopic Scores:     ", ", ".join(row['BERTopic Scores']))

print("\nKeyword data extracted.")

import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd # Ensure pandas is imported

# Assume 'lda_topics' and 'bertopic_topics' dictionaries
# contain the {word: score} mappings from the previous step

def plot_keyword_comparison(topic_id, lda_topic_data, bertopic_topic_data, num_keywords=10, title_suffix=""):
    lda_data = lda_topic_data.get(topic_id, {})
    bertopic_data = bertopic_topic_data.get(topic_id, {})

    if not lda_data or not bertopic_data:
        print(f"Data not found for Topic {topic_id}. Cannot plot.")
        return

    # Sort by score and get top N
    lda_sorted = sorted(lda_data.items(), key=lambda item: item[1], reverse=True)[:num_keywords]
    bertopic_sorted = sorted(bertopic_data.items(), key=lambda item: item[1], reverse=True)[:num_keywords]

    lda_words = [item[0] for item in lda_sorted]
    lda_scores = [item[1] for item in lda_sorted]
    bertopic_words = [item[0] for item in bertopic_sorted]
    bertopic_scores = [item[1] for item in bertopic_sorted]

    # Create subplots
    fig = make_subplots(rows=1, cols=2,
                        subplot_titles=(f"LDA: Top {num_keywords} Keywords", f"BERTopic: Top {num_keywords} Keywords"),
                        shared_yaxes=False) # Y-axes represent different scales (probability vs. c-TF-IDF)

    # Add LDA Bar Chart
    fig.add_trace(
        go.Bar(y=['<b>'+word+'</b>' for word in lda_words], # Use list comprehension for bolding
               x=lda_scores,
               orientation='h',
               marker_color='#1f77b4', # Blue
               name='LDA'),
        row=1, col=1
    )

    # Add BERTopic Bar Chart
    fig.add_trace(
        go.Bar(y=['<b>'+word+'</b>' for word in bertopic_words], # Use list comprehension for bolding
               x=bertopic_scores,
               orientation='h',
               marker_color='#ff7f0e', # Orange
               name='BERTopic'),
        row=1, col=2
    )

    # Update layout
    fig.update_layout(
        title_text=f'<b>Keyword Comparison for Topic {topic_id}</b>{title_suffix}',
        title_x=0.5, # Center title
        bargap=0.1,
        height=400, # Adjust height as needed
        width=900,  # Adjust width as needed
        showlegend=False,
        yaxis=dict(autorange="reversed"), # Show top words at the top for LDA
        yaxis2=dict(autorange="reversed") # Show top words at the top for BERTopic
    )
    fig.update_xaxes(title_text="LDA Score (Probability)", row=1, col=1)
    fig.update_xaxes(title_text="BERTopic Score (c-TF-IDF)", row=1, col=2)

    fig.show()

# --- Example: Plot comparison for Topic 1 ---
# Assuming 'lda_topics' and 'bertopic_topics' dicts are populated from previous step
print(f"\n--- Generating Keyword Bar Chart Comparison for Topic 1 ---")
plot_keyword_comparison(topic_id=1, lda_topic_data=lda_topics, bertopic_topic_data=bertopic_topics)

# --- Example: Plot comparison for Topic 8 ---
print(f"\n--- Generating Keyword Bar Chart Comparison for Topic 8 ---")
plot_keyword_comparison(topic_id=8, lda_topic_data=lda_topics, bertopic_topic_data=bertopic_topics)



# --- Step 1: Find Topics for YOUR Custom Text ---

# Define your custom text string here:
my_custom_text = "foreign deplomacy can be avoided"


print(f"\n--- Finding topics similar to Custom Text ---")
print(f"Original custom text: {my_custom_text}")

# Apply the *same* preprocessing steps used for the training data
try:
    # Assuming 'remove_html_tags' wasn't needed for LDA input based on V2 code,
    # but applying 'clean_text' (lowercase, remove special chars) and 'lemmatize_text' (spaCy)
    processed_custom_text = lemmatize_text(clean_text(my_custom_text))
    print(f"Processed custom text: {processed_custom_text}")
except NameError as e:
     print(f"Error: Preprocessing function like 'clean_text' or 'lemmatize_text' not defined. Using raw custom text.")
     print(f"Accuracy might be reduced. Error detail: {e}")
     processed_custom_text = my_custom_text # Fallback to raw text if functions aren't available

# Specify how many top similar topics you want to find
num_similar_topics = 3

# Use your trained model (topic_model_bertopic) to find the topics for the processed text
try:
    # Pass the single processed string
    similar_topics, similarity_scores = topic_model_bertopic.find_topics(processed_custom_text, top_n=num_similar_topics)

    # Print the results
    print(f'\nTop {len(similar_topics)} similar topics found: {similar_topics}')
    print(f'Similarity scores: {similarity_scores}')

    # Print the top keywords for these similar topics
    print("\n--- Top Keywords for these Similar Topics ---")
    for topic_id, score in zip(similar_topics, similarity_scores):
        print(f'\nTopic {topic_id} (Score: {score:.4f}):')
        keywords = topic_model_bertopic.get_topic(topic_id)
        if keywords:
            print(keywords)
        else:
             print(f"Could not retrieve keywords for Topic {topic_id}.")

except Exception as e:
    print(f"\nError during find_topics: {e}")

# --- The rest of the code (Saving/Loading) remains the same ---

# Define a descriptive filename for your model
model_save_path = "presidential_speeches_kmeans_bertopic_model"
print(f"\n--- Saving BERTopic model to: {model_save_path} ---")
try:
    topic_model_bertopic.save(model_save_path, serialization="safetensors", save_ctfidf=True)
    print("Model saved successfully.")
except Exception as e:
    print(f"Error saving model: {e}")

